{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook evaluates a grid run of multiple models using pre-defined metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats \n",
    "import h5py\n",
    "import loss\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzoo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_mse(a, b):\n",
    "    return ((a - b)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dir = 'datasets/ATAC_bin/grid4'\n",
    "preds = [os.path.join(grid_dir, file) for file in os.listdir(grid_dir) if (file.endswith('.h5') and 'pred_' in file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File(pred, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = dataset['test_pred'][:]\n",
    "test_y = dataset['test_y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np_mse(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65843824, 0.57946038, 0.67114482, 0.66405675, 0.65058335,\n",
       "       0.64318487, 0.64461434, 0.59419452, 0.62060418, 0.64374626,\n",
       "       0.63448487, 0.65220019, 0.65092853, 0.60915475, 0.62014465,\n",
       "       0.63481027, 0.64856279, 0.63178551, 0.61218004, 0.64591851,\n",
       "       0.59147649, 0.60651696, 0.60762309, 0.58107284])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy_pr(test_y[:,:,:], test_pred[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_pr(y_true, y_pred):\n",
    "    _,_,T = y_true.shape\n",
    "    res_pr = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        pr = scipy.stats.pearsonr(y_true[:,:,t].flatten(), y_pred[:,:,t].flatten())[0]\n",
    "        res_pr[t] = pr\n",
    "    return res_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_poiss(y_true, y_pred):\n",
    "    return np.nanmean(y_pred - y_true * np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_pois(y_true, y_pred):\n",
    "    _,_,T = y_true.shape\n",
    "    res_pr = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        pr = np_poiss(y_true[:,:,t].flatten(), y_pred[:,:,t].flatten())\n",
    "        res_pr[t] = np.nanmean(pr) \n",
    "#         print(pr)\n",
    "#         break\n",
    "    return res_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    mse = np_mse(test_y, test_pred)\n",
    "    pr = scipy_pr(test_y, test_pred)\n",
    "    pois = np_poiss(test_y, test_pred)\n",
    "    return ({'mse': mse,\n",
    "            'pr': pr,\n",
    "             'pois': pois})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shush/tf_2/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log\n",
      "  \n",
      "/home/shush/tf_2/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/home/shush/tf_2/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "eval_metrics = {}\n",
    "for pred in preds:\n",
    "    dataset = h5py.File(pred, 'r')\n",
    "    test_pred = dataset['test_pred'][:]\n",
    "    test_y = dataset['test_y'][:]\n",
    "    eval_metrics[pred] = get_metrics(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
